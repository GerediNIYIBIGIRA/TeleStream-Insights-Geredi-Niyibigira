# ğŸš€ TeleStream Insights Hub

> **Production-grade data engineering platform for telecom analytics** - Real-time & batch processing with modern data stack

[![CI/CD](https://github.com/yourorg/telestream/workflows/CI/badge.svg)](https://github.com/yourorg/telestream/actions)
[![Code Coverage](https://codecov.io/gh/yourorg/telestream/branch/main/graph/badge.svg)](https://codecov.io/gh/yourorg/telestream)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Architecture](#architecture)
- [Features](#features)
- [Prerequisites](#prerequisites)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Configuration](#configuration)
- [Running the Pipeline](#running-the-pipeline)
- [Testing](#testing)
- [Monitoring](#monitoring)
- [API Documentation](#api-documentation)
- [Contributing](#contributing)
- [License](#license)

---

## ğŸ¯ Overview

TeleStream Insights Hub is an end-to-end data platform that processes **10M+ CDR records daily** with both batch and streaming capabilities. Built for a telecom enterprise undergoing digital transformation.

### Key Capabilities

- âœ… **Dual Processing**: Batch (Spark) + Streaming (Kafka/Flink)
- âœ… **Lakehouse Architecture**: Delta Lake with Bronze-Silver-Gold layers
- âœ… **Production-Grade**: CI/CD, testing, monitoring, data quality
- âœ… **Real-time Analytics**: Sub-30s latency for network events
- âœ… **Self-Service**: REST API + dashboards for business users
- âœ… **Scalable**: Processes 100K+ events/sec with auto-scaling

### Use Cases

1. **Customer Usage Analytics** - Track data/voice/SMS consumption
2. **Network Performance Monitoring** - Real-time cell tower health
3. **Anomaly Detection** - Identify unusual usage patterns
4. **Revenue Optimization** - Customer segmentation and targeting
5. **Network Capacity Planning** - Predictive analytics

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Billing   â”‚â”€â”€â”€â–¶â”‚   Ingestion  â”‚â”€â”€â”€â–¶â”‚  Data Lake  â”‚
â”‚   System    â”‚    â”‚   (Batch/    â”‚    â”‚   (Delta)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Stream)    â”‚    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚   Network   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â–¼
â”‚  Monitoring â”‚           â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Processing     â”‚
                                    â”‚  (Spark/Kafka)  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   Customer  â”‚                              â”‚
â”‚     CRM     â”‚                              â–¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                  â”‚   Warehouse +    â”‚
                                  â”‚     Cache        â”‚
                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                           â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                      â–¼                  â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   API    â”‚          â”‚Dashboard â”‚      â”‚Notebooks â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Technology Stack**:
- **Ingestion**: Kafka, Spark Batch
- **Processing**: PySpark, Spark Structured Streaming
- **Storage**: Delta Lake, PostgreSQL, Redis
- **Orchestration**: Apache Airflow
- **API**: FastAPI (async)
- **Analytics**: Metabase, Jupyter
- **Monitoring**: Prometheus, Grafana, ELK
- **Deployment**: Docker, Kubernetes, Terraform

---

## âœ¨ Features

### Data Engineering
- ğŸ”„ **Medallion Architecture** - Bronze/Silver/Gold layers
- ğŸ¯ **Schema Evolution** - Backward compatible changes
- ğŸ” **Data Quality** - Great Expectations integration
- ğŸ“Š **Star Schema** - Optimized dimensional model
- ğŸ• **Time Travel** - Delta Lake versioning
- ğŸ” **Data Lineage** - OpenLineage tracking

### Real-Time Processing
- âš¡ **Sub-minute Latency** - Stream processing with Kafka
- ğŸ“ˆ **Windowed Aggregations** - Tumbling/sliding windows
- ğŸš¨ **Anomaly Detection** - Statistical models
- ğŸ’¾ **Stateful Processing** - RocksDB state store
- ğŸª **Exactly-Once Semantics** - Guaranteed delivery

### API & Analytics
- ğŸš€ **High-Performance API** - Async FastAPI with caching
- ğŸ”‘ **Authentication** - API keys + JWT tokens
- â±ï¸ **Rate Limiting** - 100 req/min per key
- ğŸ“± **OpenAPI Docs** - Interactive Swagger UI
- ğŸ“Š **BI Dashboards** - Metabase integration

### DevOps & Observability
- ğŸ”„ **CI/CD Pipeline** - GitHub Actions
- ğŸ§ª **Comprehensive Testing** - Unit, integration, load tests
- ğŸ“ˆ **Monitoring** - Prometheus + Grafana
- ğŸ“ **Logging** - ELK Stack
- ğŸš¨ **Alerting** - PagerDuty, Slack integration

---

## ğŸ“¦ Prerequisites

### Required Software
```bash
# Core dependencies
- Docker 20.10+
- Docker Compose 2.0+
- Python 3.9+
- Make (optional)

# For local development
- Apache Spark 3.5+
- Java 11
- Poetry or pip
```

### Cloud Resources (Optional)
- AWS Account (S3, EMR) or
- GCP Account (GCS, Dataproc) or
- Azure Account (Blob Storage, HDInsight)

---

## ğŸš€ Quick Start

### 1. Clone Repository
```bash
git clone https://github.com/yourorg/telestream-insights-hub.git
cd telestream-insights-hub
```

### 2. Set Up Environment
```bash
# Copy environment template
cp .env.example .env

# Edit configuration
vim .env

# Install Python dependencies
pip install -r requirements.txt

# Or using Poetry
poetry install
```

### 3. Start Infrastructure
```bash
# Start all services with Docker Compose
docker-compose up -d

# Verify services are running
docker-compose ps

# Check logs
docker-compose logs -f
```

### 4. Initialize Database
```bash
# Run database migrations
docker-compose exec postgres psql -U telestream -d telestream_dw -f /init-scripts/schema.sql

# Create Airflow connections
docker-compose exec airflow-webserver airflow connections add \
  --conn-type postgres \
  --conn-host postgres \
  --conn-schema telestream_dw \
  --conn-login telestream \
  --conn-password telestream123 \
  telestream_warehouse
```

### 5. Access Services

| Service | URL | Credentials |
|---------|-----|-------------|
| Airflow | http://localhost:8080 | admin / admin |
| Metabase | http://localhost:3000 | admin / admin123 |
| API Docs | http://localhost:8000/api/docs | API Key required |
| Grafana | http://localhost:3001 | admin / admin123 |
| MinIO Console | http://localhost:9001 | minioadmin / minioadmin123 |

---

## ğŸ“ Project Structure

```
tele-stream-insights-hub/
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ batch_ingest.py           # Batch data ingestion
â”‚   â””â”€â”€ kafka_producer.py         # Stream event producer
â”œâ”€â”€ processing/
â”‚   â”œâ”€â”€ batch_transform.py        # Spark batch ETL
â”‚   â””â”€â”€ streaming_processor.py    # Kafka streaming pipeline
â”œâ”€â”€ storage/
â”‚   â”œâ”€â”€ warehouse_loader.py       # Data warehouse loader
â”‚   â””â”€â”€ lake_writer.py            # Delta Lake writer
â”œâ”€â”€ orchestration/
â”‚   â”œâ”€â”€ airflow_dag.py            # Production DAG
â”‚   â””â”€â”€ prefect_flow.py           # Alternative orchestration
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ fastapi_server.py         # REST API service
â”‚   â”œâ”€â”€ models.py                 # Pydantic models
â”‚   â””â”€â”€ auth.py                   # Authentication
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/                     # Unit tests
â”‚   â”œâ”€â”€ integration/              # Integration tests
â”‚   â””â”€â”€ performance/              # Load tests
â”œâ”€â”€ dashboard/
â”‚   â”œâ”€â”€ metabase_config.json      # Dashboard configs
â”‚   â””â”€â”€ superset_dashboards/      # Apache Superset
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory_analysis.ipynb
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ HLD.md                    # High-level design
â”‚   â”œâ”€â”€ LLD.md                    # Low-level design
â”‚   â””â”€â”€ API.md                    # API documentation
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus.yml            # Metrics config
â”‚   â”œâ”€â”€ grafana_dashboards/       # Dashboard JSONs
â”‚   â””â”€â”€ alerts.yml                # Alert rules
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ terraform/                # Infrastructure as Code
â”‚   â”‚   â”œâ”€â”€ aws/
â”‚   â”‚   â”œâ”€â”€ gcp/
â”‚   â”‚   â””â”€â”€ azure/
â”‚   â””â”€â”€ kubernetes/               # K8s manifests
â”‚       â”œâ”€â”€ deployments/
â”‚       â”œâ”€â”€ services/
â”‚       â””â”€â”€ helm/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml                # CI pipeline
â”‚       â”œâ”€â”€ cd.yml                # CD pipeline
â”‚       â””â”€â”€ tests.yml             # Test automation
â”œâ”€â”€ docker-compose.yml            # Local development
â”œâ”€â”€ Dockerfile                    # Multi-stage build
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ pyproject.toml                # Poetry config
â”œâ”€â”€ setup.py                      # Package setup
â”œâ”€â”€ Makefile                      # Common commands
â””â”€â”€ README.md                     # This file
```

---

## âš™ï¸ Configuration

### Environment Variables

```bash
# Database Configuration
DB_HOST=postgres
DB_PORT=5432
DB_NAME=telestream_dw
DB_USER=telestream
DB_PASSWORD=telestream123

# Kafka Configuration
KAFKA_BROKERS=kafka:9092
KAFKA_SCHEMA_REGISTRY=http://schema-registry:8081

# Storage Configuration
DATALAKE_PATH=s3://telestream-datalake
WAREHOUSE_PATH=s3://telestream-warehouse
AWS_REGION=us-east-1

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
API_KEY_SALT=your-secret-salt

# Monitoring
PROMETHEUS_URL=http://prometheus:9090
GRAFANA_URL=http://grafana:3000

# Airflow Configuration
AIRFLOW__CORE__EXECUTOR=CeleryExecutor
AIRFLOW__CORE__FERNET_KEY=your-fernet-key
```

### Spark Configuration

Create `spark-defaults.conf`:
```properties
spark.sql.adaptive.enabled                     true
spark.sql.adaptive.coalescePartitions.enabled  true
spark.databricks.delta.optimizeWrite.enabled   true
spark.databricks.delta.autoCompact.enabled     true
spark.sql.parquet.compression.codec            snappy
spark.default.parallelism                      200
spark.sql.shuffle.partitions                   200
```

---

## ğŸ® Running the Pipeline

### Manual Execution

#### 1. Batch Ingestion
```bash
# Ingest CDR data
python processing/batch_transform.py \
  --env prod \
  --action ingest \
  --source-path s3://raw-data/cdr/2025-10-20/ \
  --batch-id BATCH-20251020-001

# Check logs
tail -f logs/batch_transform.log
```

#### 2. Stream Processing
```bash
# Start streaming pipeline
python processing/streaming_processor.py \
  --mode consume \
  --kafka-brokers kafka:9092

# In another terminal, produce test events
python processing/streaming_processor.py \
  --mode produce \
  --num-cells 100 \
  --duration 60
```

#### 3. Transformations
```bash
# Bronze to Silver
python processing/batch_transform.py \
  --env prod \
  --action transform \
  --date 2025-10-20

# Silver to Gold
python processing/batch_transform.py \
  --env prod \
  --action aggregate \
  --date 2025-10-20
```

### Using Airflow

```bash
# Trigger DAG manually
docker-compose exec airflow-webserver \
  airflow dags trigger telestream_cdr_batch_pipeline \
  --conf '{"date": "2025-10-20"}'

# Monitor DAG run
docker-compose exec airflow-webserver \
  airflow dags list-runs -d telestream_cdr_batch_pipeline

# View task logs
docker-compose exec airflow-webserver \
  airflow tasks logs telestream_cdr_batch_pipeline \
  transform_bronze_to_silver \
  2025-10-20
```

### Using Make Commands

```bash
# Run complete pipeline
make run-pipeline DATE=2025-10-20

# Run tests
make test

# Start services
make up

# Stop services
make down

# View logs
make logs SERVICE=airflow-scheduler

# Clean all data
make clean
```

---

## ğŸ§ª Testing

### Run All Tests
```bash
# Using pytest
pytest tests/ -v --cov=processing --cov-report=html

# Using Make
make test

# Run specific test suite
pytest tests/unit/ -v
pytest tests/integration/ -v -m integration
pytest tests/performance/ -v -m performance
```

### Test Categories

#### Unit Tests
```bash
# Test data transformations
pytest tests/unit/test_transformations.py -v

# Test data quality rules
pytest tests/unit/test_data_quality.py -v
```

#### Integration Tests
```bash
# Test end-to-end pipeline
pytest tests/integration/test_pipeline.py -v -m integration

# Test API endpoints
pytest tests/integration/test_api.py -v
```

#### Performance Tests
```bash
# Load test API
pytest tests/performance/test_load.py -v -m performance

# Benchmark Spark jobs
pytest tests/performance/test_spark_perf.py -v
```

### Code Quality

```bash
# Linting
flake8 processing/ api/ --max-line-length=120

# Type checking
mypy processing/ api/ --ignore-missing-imports

# Security scan
bandit -r processing/ api/

# Format code
black processing/ api/
isort processing/ api/
```

---

## ğŸ“Š Monitoring

### Metrics Dashboard

Access Grafana at `http://localhost:3001` to view:

1. **Pipeline Metrics**
   - Records processed per minute
   - Processing latency (P50, P95, P99)
   - Error rates
   - Data quality scores

2. **Infrastructure Metrics**
   - CPU/Memory utilization
   - Disk I/O
   - Network throughput
   - Kafka lag

3. **Business Metrics**
   - Total revenue
   - Active customers
   - Network health score
   - Anomaly count

### Alerts

Configured alerts trigger on:
- Pipeline failure (PagerDuty)
- Data quality < 95% (Slack)
- API latency > 1s (Email)
- Disk usage > 80% (Slack)

---

## ğŸ“š API Documentation

### Authentication

```bash
# Get API key (admin only)
curl -X POST http://localhost:8000/api/v1/auth/keys \
  -H "Authorization: Bearer <admin-token>" \
  -d '{"name": "my-app", "role": "analyst"}'
```

### Example Requests

```bash
# Get real-time metrics
curl http://localhost:8000/api/v1/metrics/realtime \
  -H "X-API-Key: test-api-key-12345"

# Get customer usage
curl "http://localhost:8000/api/v1/customers/CUST-0001/usage?start_date=2025-01-01&end_date=2025-01-31" \
  -H "X-API-Key: test-api-key-12345"

# Get top customers
curl "http://localhost:8000/api/v1/customers/top?limit=10&order_by=revenue" \
  -H "X-API-Key: test-api-key-12345"

# Get network performance
curl "http://localhost:8000/api/v1/network/performance?technology=5G&min_health_score=50" \
  -H "X-API-Key: test-api-key-12345"
```

Full API documentation: http://localhost:8000/api/docs

---

## ğŸš¢ Deployment

### Docker

```bash
# Build image
docker build -t telestream:latest .

# Push to registry
docker tag telestream:latest your-registry/telestream:v1.0.0
docker push your-registry/telestream:v1.0.0
```

### Kubernetes

```bash
# Deploy with Helm
helm install telestream ./infrastructure/kubernetes/helm/telestream \
  --namespace telestream \
  --create-namespace \
  --values values-prod.yaml

# Check status
kubectl get pods -n telestream
kubectl logs -f deployment/telestream-api -n telestream
```

### Terraform

```bash
# Initialize
cd infrastructure/terraform/aws
terraform init

# Plan
terraform plan -var-file=prod.tfvars

# Apply
terraform apply -var-file=prod.tfvars
```

---

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for details.

### Development Workflow

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Code Standards

- Follow PEP 8 style guide
- Write unit tests for new features
- Update documentation
- Add type hints
- Keep functions < 50 lines

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¥ Team

- **Project Lead**: Data Engineering Team
- **Contributors**: See [CONTRIBUTORS.md](CONTRIBUTORS.md)
- **Support**: data-team@telestream.com

---

## ğŸ“ Support

- **Documentation**: https://docs.telestream.io
- **Issues**: https://github.com/yourorg/telestream/issues
- **Slack**: #telestream-support
- **Email**: support@telestream.com

---

## ğŸ“ Learning Resources

- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [Delta Lake Guide](https://docs.delta.io/latest/index.html)
- [Airflow Best Practices](https://airflow.apache.org/docs/apache-airflow/stable/best-practices.html)
- [FastAPI Tutorial](https://fastapi.tiangolo.com/tutorial/)
- [Data Engineering Zoomcamp](https://github.com/DataTalksClub/data-engineering-zoomcamp)

---

**â­ If this project helps you, please star it on GitHub!**